{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af48be9d",
   "metadata": {},
   "source": [
    "# A2 - Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21364db5",
   "metadata": {},
   "source": [
    "In this assignment, I have build a neural language model that learns the structure, context, and style of a given text corpus and generates coherent text continuations.\n",
    "I have implemented a Long Short-Term Memory (LSTM) based language model and evaluate it using perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61cc52",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11cfa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import Vocab\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Checking for GPU\n",
    "print(\"Device:\", device)\n",
    "\n",
    "SEED = 1234 # Setting seed for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9860d25",
   "metadata": {},
   "source": [
    "# Task 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45007cc",
   "metadata": {},
   "source": [
    "#### Dataset Aquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d3e4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 55422\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"the-rizz/the-rizz-corpus\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c82d5",
   "metadata": {},
   "source": [
    "#### Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccd83d",
   "metadata": {},
   "source": [
    "Dataset Chosen: The Rizz Corpus\n",
    "Source: HuggingFace Datasets\n",
    "Link: https://huggingface.co/datasets/the-rizz/the-rizz-corpus\n",
    "\n",
    "The Rizz Corpus is a text-rich conversational dataset containing short dialogue-style sentences designed to represent social and persona-based language.It is a collection of informal conversations taken from various social media apps.\n",
    "It is suitable for language modeling because :\n",
    "\n",
    "* It contains natural, informal language\n",
    "\n",
    "* The text is diverse and context-dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23346d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to rizz_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Saving dataset as .txt\n",
    "\n",
    "output_path = \"rizz_corpus.txt\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in dataset[\"train\"]:\n",
    "        text = example[\"text\"].strip()\n",
    "        if text:                     # skip empty lines\n",
    "            f.write(text + \"\\n\")\n",
    "\n",
    "print(f\"Dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05731bfc",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac9682e",
   "metadata": {},
   "source": [
    "#### Dataset Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660717f9",
   "metadata": {},
   "source": [
    "Since the dataset only provides a single split, I manually divide it into:\n",
    "\n",
    "* Training set (80%)\n",
    "\n",
    "* Validation set (10%)\n",
    "\n",
    "* Test set (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5d47a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44337 5542 5543\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=SEED)\n",
    "temp = dataset[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "valid_dataset = temp[\"train\"]\n",
    "test_dataset  = temp[\"test\"]\n",
    "\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73347c61",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Before training the language model, the text data was cleaned and prepared using the following steps:\n",
    "\n",
    "* Removing empty lines and lowercasing\n",
    "* Some entries in the dataset were empty, so those were removed.\n",
    "* All text was converted to lowercase to keep the vocabulary smaller and more consistent.\n",
    "\n",
    "Tokenization - \n",
    "Each sentence was split into individual words using a simple space-based tokenizer.This converts raw text into tokens that the model can learn from.\n",
    "\n",
    "Building the vocabulary - \n",
    "A vocabulary was created from the training data only.Words that appeared very rarely were ignored to reduce noise.\n",
    "\n",
    "Two special tokens were added :\n",
    "\n",
    "* UNK for unknown words\n",
    "\n",
    "* EOS to indicate the end of a sentence\n",
    "\n",
    "Converting text to numbers (Numericalization) - \n",
    "Every word token was mapped to a unique number using the vocabulary.The <eos> token was added at the end of each sentence so the model learns when a sentence should stop.\n",
    "\n",
    "Batching the data -\n",
    "All word indices were grouped into fixed-size batches so the LSTM could be trained efficiently using sequences of equal length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a33d1",
   "metadata": {},
   "source": [
    "#### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bd7af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [ex[\"text\"].lower() for ex in train_dataset if ex[\"text\"].strip() != \"\"]\n",
    "valid_texts = [ex[\"text\"].lower() for ex in valid_dataset if ex[\"text\"].strip() != \"\"]\n",
    "test_texts  = [ex[\"text\"].lower() for ex in test_dataset  if ex[\"text\"].strip() != \"\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357c630",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6bf6fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>[inst]', 'i', 'am', 'basically', 'a', 'gym', 'rat.', 'but', 'i', 'try', 'to', 'read', 'often', 'too.[/inst]well', 'when', 'i', 'am', 'not', 'flying', 'kites', 'with', 'my', 'niece', 'i', 'am', 'at', 'the', 'gym', 'just', 'not', 'so', 'much</s>']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "train_tok = [tokenize(t) for t in train_texts]\n",
    "valid_tok = [tokenize(t) for t in valid_texts]\n",
    "test_tok  = [tokenize(t) for t in test_texts]\n",
    "\n",
    "print(train_tok[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb69065",
   "metadata": {},
   "source": [
    "#### Vocabulary Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9515fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 20178\n",
      "['<unk>', '<eos>', '<unk>', '<pad>', 'i', 'to', 'a', 'you', 'the', 'are']\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "for tokens in train_tok:\n",
    "    counter.update(tokens)\n",
    "\n",
    "# applying min_freq = 3\n",
    "counter = Counter({k: v for k, v in counter.items() if v >= 3})\n",
    "\n",
    "vocab = Vocab(counter)\n",
    "\n",
    "# End-of-Sentence Token\n",
    "vocab.itos.insert(0, \"<unk>\")\n",
    "vocab.itos.insert(1, \"<eos>\")\n",
    "vocab.stoi = {tok: i for i, tok in enumerate(vocab.itos)}\n",
    "\n",
    "UNK_IDX = vocab.stoi[\"<unk>\"]\n",
    "EOS_IDX = vocab.stoi[\"<eos>\"]\n",
    "\n",
    "# Checking vocab size and some tokens\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(vocab.itos[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb31508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detokenizer\n",
    "\n",
    "PUNCT_NO_SPACE_BEFORE = {\".\", \",\", \"!\", \"?\", \";\", \":\", \"%\", \")\", \"]\", \"}\"}\n",
    "PUNCT_NO_SPACE_AFTER  = {\"(\", \"[\", \"{\"}\n",
    "\n",
    "def decode(ids):\n",
    "    tokens = [vocab.itos[i] for i in ids]\n",
    "    out = []\n",
    "\n",
    "    for tok in tokens:\n",
    "        if not out:\n",
    "            out.append(tok)\n",
    "            continue\n",
    "\n",
    "        prev = out[-1]\n",
    "\n",
    "        if tok in PUNCT_NO_SPACE_BEFORE:\n",
    "            out[-1] = prev + tok\n",
    "        elif prev in PUNCT_NO_SPACE_AFTER:\n",
    "            out[-1] = prev + tok\n",
    "        elif tok in {\"'\", \"’\"}:\n",
    "            out[-1] = prev + tok\n",
    "        elif prev.endswith((\"'\", \"’\")):\n",
    "            out[-1] = prev + tok\n",
    "        else:\n",
    "            out.append(\" \" + tok)\n",
    "\n",
    "    return \"\".join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sampling Utilities (Top-k / Top-p)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_next_token(probs, top_k=50, top_p=0.9):\n",
    "    probs = probs.squeeze(0)\n",
    "\n",
    "    # Top-k filtering\n",
    "    if top_k is not None and top_k > 0:\n",
    "        k = min(top_k, probs.numel())\n",
    "        values, indices = torch.topk(probs, k)\n",
    "        mask = torch.zeros_like(probs)\n",
    "        mask[indices] = values\n",
    "        probs = mask / mask.sum()\n",
    "\n",
    "    # Top-p (nucleus) filtering\n",
    "    if top_p is not None and 0 < top_p < 1:\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cumulative = torch.cumsum(sorted_probs, dim=0)\n",
    "        cutoff = cumulative > top_p\n",
    "        cutoff[0] = False\n",
    "        sorted_probs[cutoff] = 0\n",
    "        probs = torch.zeros_like(probs)\n",
    "        probs[sorted_idx] = sorted_probs\n",
    "        probs = probs / probs.sum()\n",
    "\n",
    "    return torch.multinomial(probs, 1).item() # Sampling from the filtered distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be3a7b",
   "metadata": {},
   "source": [
    "#### Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80bc5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numericalize(tokens):\n",
    "    return [vocab.stoi.get(tok, UNK_IDX) for tok in tokens]\n",
    "\n",
    "def build_data(token_lists):\n",
    "    ids = []\n",
    "    for tokens in token_lists:\n",
    "        tokens = tokens + [\"<eos>\"]\n",
    "        ids.extend(numericalize(tokens))\n",
    "    return torch.LongTensor(ids)\n",
    "\n",
    "train_ids = build_data(train_tok)\n",
    "valid_ids = build_data(valid_tok)\n",
    "test_ids  = build_data(test_tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81f8d2",
   "metadata": {},
   "source": [
    "#### Batch Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for batching\n",
    "def get_data(data, batch_size):\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    return data.view(batch_size, num_batches)\n",
    "\n",
    "batch_size = 128 # Adjusts as whatever we requiree\n",
    "\n",
    "train_data = get_data(train_ids, batch_size) \n",
    "valid_data = get_data(valid_ids, batch_size) \n",
    "test_data  = get_data(test_ids,  batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a2401",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e70dd8",
   "metadata": {},
   "source": [
    "#### Model Architecture - \n",
    "\n",
    "The language model is built using an LSTM-based neural network with the following structure:\n",
    "\n",
    "* Embedding Layer\n",
    "Converts word indices into dense vectors so the model can understand relationships between words.\n",
    "\n",
    "* LSTM Layers\n",
    "The LSTM processes sequences of words and learns the context of a sentence by remembering past information.\n",
    "Using multiple layers helps the model learn more complex patterns.\n",
    "\n",
    "* Dropout Layers\n",
    "Dropout is applied to reduce overfitting and improve generalization on unseen text.\n",
    "\n",
    "* Output Linear Layer\n",
    "This layer maps the LSTM outputs to the size of the vocabulary, producing predictions for the next word.\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "* The model was trained using cross-entropy loss, which is suitable for next-word prediction tasks.\n",
    "\n",
    "* Adam optimizer was used to update the model parameters.\n",
    "\n",
    "* Training was done on fixed-length sequences using truncated backpropagation through time.\n",
    "\n",
    "* Gradient clipping was applied to avoid unstable training.\n",
    "\n",
    "* After each epoch, the model was evaluated on the validation set.\n",
    "\n",
    "The model with the best validation performance was saved and later used for testing and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63163f9",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b43c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hid_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "    \n",
    "    def init_hidden(self, batch_size, device): \n",
    "        h = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        c = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return h, c\n",
    "\n",
    "    def detach_hidden(self, hidden):\n",
    "        return hidden[0].detach(), hidden[1].detach()\n",
    "    \n",
    "    def forward(self, src, hidden):\n",
    "        emb = self.dropout(self.embedding(src))\n",
    "        out, hidden = self.lstm(emb, hidden)\n",
    "        out = self.dropout(out)\n",
    "        pred = self.fc(out)\n",
    "        return pred, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56bb01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates input–target pairs for language modeling by shifting the sequence by one token\n",
    "\n",
    "def get_batch(data, seq_len, idx):\n",
    "    src = data[:, idx:idx+seq_len]\n",
    "    tgt = data[:, idx+1:idx+seq_len+1]\n",
    "    return src, tgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c200cfff",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884552b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Adjusting data size to be multiple of seq_len\n",
    "\n",
    "    num_batches = data.shape[1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[1]\n",
    "\n",
    "    # Initializing hidden state\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    for idx in range(0, num_batches - 1, seq_len):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, tgt = get_batch(data, seq_len, idx)\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        pred, hidden = model(src, hidden)\n",
    "        pred = pred.reshape(batch_size * seq_len, -1)\n",
    "        tgt = tgt.reshape(-1)\n",
    "\n",
    "        loss = criterion(pred, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "\n",
    "    return epoch_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "\n",
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    num_batches = data.shape[1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    # Disabling gradient calculation for evaluation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, tgt = get_batch(data, seq_len, idx)\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            pred, hidden = model(src, hidden)\n",
    "            pred = pred.reshape(batch_size * seq_len, -1)\n",
    "            tgt = tgt.reshape(-1)\n",
    "\n",
    "            loss = criterion(pred, tgt)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "\n",
    "    return epoch_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9653b33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "  Train Perplexity: 166.564\n",
      "  Valid Perplexity: 61.106\n",
      "Epoch 2\n",
      "  Train Perplexity: 67.926\n",
      "  Valid Perplexity: 47.294\n",
      "Epoch 3\n",
      "  Train Perplexity: 55.711\n",
      "  Valid Perplexity: 41.691\n",
      "Epoch 4\n",
      "  Train Perplexity: 49.132\n",
      "  Valid Perplexity: 38.438\n",
      "Epoch 5\n",
      "  Train Perplexity: 44.700\n",
      "  Valid Perplexity: 36.344\n",
      "Epoch 6\n",
      "  Train Perplexity: 41.559\n",
      "  Valid Perplexity: 34.996\n",
      "Epoch 7\n",
      "  Train Perplexity: 39.075\n",
      "  Valid Perplexity: 33.997\n",
      "Epoch 8\n",
      "  Train Perplexity: 37.121\n",
      "  Valid Perplexity: 33.183\n",
      "Epoch 9\n",
      "  Train Perplexity: 35.501\n",
      "  Valid Perplexity: 32.594\n",
      "Epoch 10\n",
      "  Train Perplexity: 34.106\n",
      "  Valid Perplexity: 32.158\n",
      "Epoch 11\n",
      "  Train Perplexity: 32.890\n",
      "  Valid Perplexity: 31.868\n",
      "Epoch 12\n",
      "  Train Perplexity: 31.816\n",
      "  Valid Perplexity: 31.637\n",
      "Epoch 13\n",
      "  Train Perplexity: 30.843\n",
      "  Valid Perplexity: 31.478\n",
      "Epoch 14\n",
      "  Train Perplexity: 29.948\n",
      "  Valid Perplexity: 31.326\n",
      "Epoch 15\n",
      "  Train Perplexity: 29.195\n",
      "  Valid Perplexity: 31.235\n",
      "Epoch 16\n",
      "  Train Perplexity: 28.474\n",
      "  Valid Perplexity: 31.179\n",
      "Epoch 17\n",
      "  Train Perplexity: 27.799\n",
      "  Valid Perplexity: 31.159\n",
      "Epoch 18\n",
      "  Train Perplexity: 27.187\n",
      "  Valid Perplexity: 31.188\n",
      "Epoch 19\n",
      "  Train Perplexity: 26.289\n",
      "  Valid Perplexity: 31.020\n",
      "Epoch 20\n",
      "  Train Perplexity: 25.830\n",
      "  Valid Perplexity: 30.977\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters :\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "emb_dim = 1024          # 1024\n",
    "hid_dim = 1024          # 1024\n",
    "num_layers = 2          # 2\n",
    "dropout = 0.65          # 0.65\n",
    "lr = 1e-3               # 1e-3\n",
    "\n",
    "# Model Initialization\n",
    "\n",
    "model = LSTMLanguageModel(\n",
    "    vocab_size, emb_dim, hid_dim, num_layers, dropout\n",
    ").to(device)\n",
    "\n",
    "# Setting up optimizer and loss function\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training Configuration\n",
    "\n",
    "n_epochs = 20           # 20 epochs\n",
    "seq_len = 50            # decoding length = 50\n",
    "clip = 0.25             # gradient clipping\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.5, patience=0\n",
    ")\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(\n",
    "        model, train_data, optimizer,\n",
    "        criterion, batch_size,\n",
    "        seq_len, clip, device\n",
    "    )\n",
    "\n",
    "    valid_loss = evaluate(\n",
    "        model, valid_data, criterion,\n",
    "        batch_size, seq_len, device\n",
    "    )\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"best-val-lstm_lm.pt\") # saving the best model\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"  Train Perplexity: {math.exp(train_loss):.3f}\")\n",
    "    print(f\"  Valid Perplexity: {math.exp(valid_loss):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ba4ddc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vocab.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Saving Vocabulary for Inference / Web App\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"stoi\": vocab.stoi,\n",
    "        \"itos\": vocab.itos\n",
    "    },\n",
    "    \"vocab.pt\"\n",
    ")\n",
    "\n",
    "print(\"Saved vocab.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbde262d",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10e063c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 31.911\n"
     ]
    }
   ],
   "source": [
    "# Loading the best model saved during training\n",
    "model.load_state_dict(torch.load(\"best-val-lstm_lm.pt\", map_location=device))\n",
    "\n",
    "# Evaluating the trained model on the test dataset\n",
    "test_loss = evaluate(\n",
    "    model,          # trained LSTM language model\n",
    "    test_data,      # test dataset\n",
    "    criterion,      # loss function (CrossEntropy)\n",
    "    batch_size,     # batch size used during training\n",
    "    seq_len,        # sequence length for evaluation\n",
    "    device          # CPU is being used \n",
    ")\n",
    "\n",
    "# Converting test loss to perplexity for easier interpretation\n",
    "print(f\"Test Perplexity: {math.exp(test_loss):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67012c90",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d65fb",
   "metadata": {},
   "source": [
    "#### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92f9ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompt,\n",
    "    max_seq_len,\n",
    "    temperature,\n",
    "    model,\n",
    "    tokenizer_fn,\n",
    "    vocab,\n",
    "    device,\n",
    "    seed=None\n",
    "):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer_fn(prompt)\n",
    "    indices = [vocab.stoi.get(t, UNK_IDX) for t in tokens]\n",
    "\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "\n",
    "            probs = torch.softmax(\n",
    "                prediction[:, -1] / temperature,\n",
    "                dim=-1\n",
    "            )\n",
    "\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            while next_token == UNK_IDX:\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            if next_token == EOS_IDX:\n",
    "                break\n",
    "\n",
    "            indices.append(next_token)\n",
    "\n",
    "    tokens = [vocab.itos[i] for i in indices]\n",
    "    tokens = [t for t in tokens if t not in (\"<unk>\", \"<eos>\")]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f8a54",
   "metadata": {},
   "source": [
    "#### Sample Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87d1e52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature = 0.5\n",
      "hello there, how are you?</s>\n",
      "\n",
      "Temperature = 0.7\n",
      "hello there, how are you?</s>\n",
      "\n",
      "Temperature = 0.75\n",
      "hello there, how are you?</s>\n",
      "\n",
      "Temperature = 0.8\n",
      "hello there, how are you?</s>\n",
      "\n",
      "Temperature = 1.0\n",
      "hello there, where are you from?</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"hello there,\"\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "\n",
    "for t in temperatures:\n",
    "    print(f\"\\nTemperature = {t}\")\n",
    "    output = generate(\n",
    "        prompt=prompt,\n",
    "        max_seq_len=max_seq_len,\n",
    "        temperature=t,\n",
    "        model=model,\n",
    "        tokenizer_fn=tokenize,\n",
    "        vocab=vocab,\n",
    "        device=device,\n",
    "        seed=seed\n",
    "    )\n",
    "    print(\" \".join(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc170e",
   "metadata": {},
   "source": [
    "#### Description\n",
    "\n",
    "During inference, the trained LSTM language model is used to generate text autoregressively.  \n",
    "Given an input prompt, the text is first tokenized and converted into numerical indices using the\n",
    "trained vocabulary. These indices are then passed through the LSTM model to predict the probability\n",
    "distribution of the next word.\n",
    "\n",
    "At each step, the next word is sampled from this distribution (optionally using temperature and\n",
    "sampling strategies) and appended back to the input sequence. This process is repeated until the\n",
    "maximum generation length is reached or an end-of-sentence token is produced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9099c",
   "metadata": {},
   "source": [
    "#### Training Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a01d83",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The model shows rapid improvement during the early epochs, with validation perplexity dropping\n",
    "significantly from **61.11 (Epoch 1)** to around **32 by Epoch 10**, indicating that the LSTM quickly\n",
    "learns the basic structure and patterns of the language. This phase reflects effective learning and\n",
    "good alignment between training and validation performance.\n",
    "\n",
    "After approximately **Epoch 12**, validation perplexity stabilizes around **31**, while training\n",
    "perplexity continues to decrease gradually. This suggests diminishing returns from additional\n",
    "training and the onset of mild overfitting. Overall, the results demonstrate successful convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b18fe",
   "metadata": {},
   "source": [
    "#### Web Application – Model Interface (Summary)\n",
    "\n",
    "The web application provides a simple interface for interacting with the trained LSTM-based language\n",
    "model. It is built using Flask and allows users to enter a text prompt through a web form. The backend\n",
    "loads the trained PyTorch model and the saved vocabulary to ensure that preprocessing during inference\n",
    "matches the training setup.\n",
    "\n",
    "When a user submits a prompt, it is tokenized, converted into numerical indices, and passed to the\n",
    "LSTM model for autoregressive text generation. The model predicts the next token step by step, applies\n",
    "temperature-based sampling, and generates a coherent continuation of the input text. The generated\n",
    "output is converted back to readable text and displayed on the webpage in real time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
